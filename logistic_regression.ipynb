{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will be implementing a Logistic Regression from scratch, just using numpy and the math behind a logistic regression. I will be using the sample Titanic dataset from seaborn and comparing it to the logistic regression model from `statsmodels` to see whether my algorithm works correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the Titanic dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity, we will use only a subset of the columns - pclass, age, fare, sibsp and parch\n",
    "X = np.array(titanic[['pclass', 'age', 'fare', 'sibsp', 'parch']].dropna())\n",
    "\n",
    "# standardize the data \n",
    "X_std = (X - np.mean(X, axis = 0))/np.std(X, axis = 0)\n",
    "\n",
    "# adding a bias term to the model (the statsmodels requires one)\n",
    "X_sm = sm.add_constant(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering the target variable based on our independent variables\n",
    "y = np.array(titanic['survived'][titanic[['pclass', 'age', 'fare', 'sibsp', 'parch']].dropna().index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((714, 6), (714,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 891 observations, 6 features\n",
    "X_sm.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statsmodels Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.570854\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "# let's take a look at the results of our baseline model, which we want to roughly replicate\n",
    "model = sm.Logit(y, X_sm)\n",
    "result = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>  <td>   714</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   708</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     5</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 02 Feb 2025</td> <th>  Pseudo R-squ.:     </th>  <td>0.1548</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>14:58:14</td>     <th>  Log-Likelihood:    </th> <td> -407.59</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -482.26</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.848e-30</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   -0.4301</td> <td>    0.086</td> <td>   -5.004</td> <td> 0.000</td> <td>   -0.599</td> <td>   -0.262</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>   -0.9658</td> <td>    0.122</td> <td>   -7.900</td> <td> 0.000</td> <td>   -1.205</td> <td>   -0.726</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>   -0.6469</td> <td>    0.105</td> <td>   -6.181</td> <td> 0.000</td> <td>   -0.852</td> <td>   -0.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.1742</td> <td>    0.134</td> <td>    1.299</td> <td> 0.194</td> <td>   -0.089</td> <td>    0.437</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.2716</td> <td>    0.099</td> <td>   -2.755</td> <td> 0.006</td> <td>   -0.465</td> <td>   -0.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.2114</td> <td>    0.093</td> <td>    2.273</td> <td> 0.023</td> <td>    0.029</td> <td>    0.394</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}   &        y         & \\textbf{  No. Observations:  } &      714    \\\\\n",
       "\\textbf{Model:}           &      Logit       & \\textbf{  Df Residuals:      } &      708    \\\\\n",
       "\\textbf{Method:}          &       MLE        & \\textbf{  Df Model:          } &        5    \\\\\n",
       "\\textbf{Date:}            & Sun, 02 Feb 2025 & \\textbf{  Pseudo R-squ.:     } &   0.1548    \\\\\n",
       "\\textbf{Time:}            &     14:58:14     & \\textbf{  Log-Likelihood:    } &   -407.59   \\\\\n",
       "\\textbf{converged:}       &       True       & \\textbf{  LL-Null:           } &   -482.26   \\\\\n",
       "\\textbf{Covariance Type:} &    nonrobust     & \\textbf{  LLR p-value:       } & 1.848e-30   \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "               & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const} &      -0.4301  &        0.086     &    -5.004  &         0.000        &       -0.599    &       -0.262     \\\\\n",
       "\\textbf{x1}    &      -0.9658  &        0.122     &    -7.900  &         0.000        &       -1.205    &       -0.726     \\\\\n",
       "\\textbf{x2}    &      -0.6469  &        0.105     &    -6.181  &         0.000        &       -0.852    &       -0.442     \\\\\n",
       "\\textbf{x3}    &       0.1742  &        0.134     &     1.299  &         0.194        &       -0.089    &        0.437     \\\\\n",
       "\\textbf{x4}    &      -0.2716  &        0.099     &    -2.755  &         0.006        &       -0.465    &       -0.078     \\\\\n",
       "\\textbf{x5}    &       0.2114  &        0.093     &     2.273  &         0.023        &        0.029    &        0.394     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Logit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                  714\n",
       "Model:                          Logit   Df Residuals:                      708\n",
       "Method:                           MLE   Df Model:                            5\n",
       "Date:                Sun, 02 Feb 2025   Pseudo R-squ.:                  0.1548\n",
       "Time:                        14:58:14   Log-Likelihood:                -407.59\n",
       "converged:                       True   LL-Null:                       -482.26\n",
       "Covariance Type:            nonrobust   LLR p-value:                 1.848e-30\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -0.4301      0.086     -5.004      0.000      -0.599      -0.262\n",
       "x1            -0.9658      0.122     -7.900      0.000      -1.205      -0.726\n",
       "x2            -0.6469      0.105     -6.181      0.000      -0.852      -0.442\n",
       "x3             0.1742      0.134      1.299      0.194      -0.089       0.437\n",
       "x4            -0.2716      0.099     -2.755      0.006      -0.465      -0.078\n",
       "x5             0.2114      0.093      2.273      0.023       0.029       0.394\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we define a function to turn our weights and bias into probabilities using the sigmoid function\n",
    "def sigmoid(X: np.array, weights: np.array, bias: int):\n",
    "    probabilities = 1/(1 + np.exp(-(np.dot(X, weights.T) + bias)))\n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a predictions function based on our generated probabilities\n",
    "def predictions(X: np.array, weights: np.array, bias: int):\n",
    "    probabilities = sigmoid(X, weights, bias)\n",
    "    predictions = np.round(probabilities)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a log loss function here just to track how our log regression is doing over time\n",
    "def log_loss(y, y_prob):\n",
    "    m = y.shape[0]\n",
    "    return (1/m)*np.sum(-y*np.log(y_prob) - (1-y)*np.log(1-y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our choice of algorithm for minimizing the loss function is stochastic gradient descent - as an exercise in the future, can implement multiple algorithms\n",
    "def stochastic_gradient_descent(y, X, weights, bias, learning_rate):\n",
    "    # iterate over all the possible data points:\n",
    "    m = X.shape[0]\n",
    "    for i in range(m):\n",
    "        # we select a random X, y tuple from the input data\n",
    "        random_index = np.random.randint(m)\n",
    "        X_i = X[random_index]\n",
    "        y_i = y[random_index]\n",
    "\n",
    "        # get the initial probabilities based on our weights\n",
    "        y_prob = sigmoid(X_i, weights, bias)\n",
    "\n",
    "        # get the gradients of the weights and the bias - based on partial derivatives of log loss (calculated outside this notebook) - these will be the numbers, by which we need to update the weights and bias\n",
    "        d_weights  = -(y_i - y_prob)*X_i\n",
    "        d_bias = -(y_i - y_prob)\n",
    "\n",
    "        weights = weights - learning_rate*d_weights\n",
    "        bias = bias - learning_rate*d_bias\n",
    "\n",
    "    return weights, bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X, y, learning_rate = 0.001, n_iterations = 100):\n",
    "    # first we initialize the weights and bias of our logistic regression model - here I will do 0, but I can also test out random initialization later\n",
    "    weights = np.zeros(X.shape[1])\n",
    "    bias = 0\n",
    "\n",
    "    loss_tracker = []\n",
    "\n",
    "    # now we loop over the defined number of iterations\n",
    "    for iteration in range(n_iterations):\n",
    "        # update the weights and bias using stochastic gradient descent every iteration:\n",
    "        weights, bias = stochastic_gradient_descent(y, X, weights, bias, learning_rate)\n",
    "\n",
    "        # calculate the probabilities after each SGD\n",
    "        y_prob = sigmoid(X, weights, bias)\n",
    "        loss_tracker.append(log_loss(y, y_prob))\n",
    "    \n",
    "    return weights, bias, loss_tracker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias, loss_tracker = logistic_regression(X_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here we plot our log loss over the iterations/epochs to visualize the convergence\n",
    "sns.lineplot(loss_tracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.45450695, -0.92872329, -0.63991956,  0.14326792, -0.28276259,\n",
       "        0.19869502])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_results = np.append(np.array(bias), weights)\n",
    "my_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.43006148, -0.96583151, -0.64693263,  0.17421386, -0.27155985,\n",
       "        0.21136566])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_results = result.params\n",
    "sm_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the two models, the coefficients are largely comparable. This was overall a success! Some things to note:\n",
    "- the coefficients change every time the code is run since the stochastic gradient descent chooses a random data point each time - the calculations always change\n",
    "- I had to decrease the learning rate from the default 0.01, because the log loss results were quite choppy otherwise - with a higher learning rate, there is a smoother curve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
