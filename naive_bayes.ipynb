{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I will be implementing a (basic) Naive Bayes classifier from scratch, just using the Bayes' Theorem. I will be testing it on the Iris dataset, which is a great beginner friendly dataset built into sklearn.\n",
    "\n",
    "In the Iris dataset, we are given 4 features (sepal length, sepal width, petal length, petal width) and are supposed to predict 1 of 3 classes based on these features - either a setosa, versicolor or virginica. We're classifying different types of flowers based on our available information. In the context of the Bayes' theorem, we are interested in computing the following probabilities for each observation *i*, which can be expressed as:\n",
    "\n",
    "$$\n",
    "P(Setosa|sepal\\ length_i \\cap sepal\\ width_i \\cap petal\\ length_i \\cap petal\\ width_i) = \\frac{P(Setosa)\\cdot P(sepal\\ length_i \\cap sepal\\ width_i \\cap petal\\ length_i \\cap petal\\ width_i|Setosa)}{P(sepal\\ length_i \\cap sepal\\ width_i \\cap petal\\ length_i \\cap petal\\ width_i)} \\\\~\\\\\n",
    "P(Versicolor|sepal\\ length_i \\cap sepal\\ width_i \\cap petal\\ length_i \\cap petal\\ width_i) = \\frac{P(Versicolor)\\cdot P(sepal\\ length_i \\cap sepal\\ width_i \\cap petal\\ length_i \\cap petal\\ width_i|Versicolor)}{P(sepal\\ length_i \\cap sepal\\ width_i \\cap petal\\ length_i \\cap petal\\ width_i)} \\\\~\\\\\n",
    "P(Virginica|sepal\\ length_i \\cap sepal\\ width_i \\cap petal\\ length_i \\cap petal\\ width_i) = \\frac{P(Virginica)\\cdot P(sepal\\ length_i \\cap sepal\\ width_i \\cap petal\\ length_i \\cap petal\\ width_i|Virginica)}{P(sepal\\ length_i \\cap sepal\\ width_i \\cap petal\\ length_i \\cap petal\\ width_i)}\n",
    "$$\n",
    "\n",
    "These original equations are quite difficult to compute as they are, as the general product rule for probabilities states that $P(A\\cap B) = P(A) \\cdot P(B|A)$. In the context of for example the first equation, this would look something like $P(sepal\\ length_i|sepal\\ width_i \\cap petal\\ length_i \\cap petal\\ width_i \\cap Setosa) \\cdot P(sepal\\ width_i|petal\\ length_i \\cap petal\\ width_i \\cap Setosa)\\ etc.$, which would require the computation of joint probabilities, which can be expensive (and for some cases may even just straight up be 0). \n",
    "\n",
    "This is why we use the naive assumption, which simplifies the product role to $P(A\\cap B) = P(A) \\cdot P(B)$. With this assumption, we assume the standalone probabilities of each feature given a particular class are independent (which for most real-world datasets is most likely wrong - e.g. in the iris dataset the petal length and petal width are most likely correlated). The above probabilities can therefore be re-written as:\n",
    "\n",
    "$$\n",
    "P(Setosa|sepal\\ length_i \\cap sepal\\ width_i \\cap petal\\ length_i \\cap petal\\ width_i) = \\frac{P(Setosa)\\cdot P(sepal\\ length_i|Setosa) \\cdot P(sepal\\ width_i|Setosa) \\cdot P(petal\\ length_i|Setosa) \\cdot P(petal\\ width_i|Setosa)}{P(sepal\\ length_i \\cap sepal\\ width_i \\cap petal\\ length_i \\cap petal\\ width_i)} \\\\~\\\\\n",
    "P(Versicolor|sepal\\ length_i \\cap sepal\\ width_i \\cap petal\\ length_i \\cap petal\\ width_i) = \\frac{P(Versicolor)\\cdot P(sepal\\ length_i|Versicolor) \\cdot P(sepal\\ width_i|Versicolor) \\cdot P(petal\\ length_i|Versicolor) \\cdot P(petal\\ width_i|Versicolor)}{P(sepal\\ length_i \\cap sepal\\ width_i \\cap petal\\ length_i \\cap petal\\ width_i)} \\\\~\\\\\n",
    "P(Virginica|sepal\\ length_i \\cap sepal\\ width_i \\cap petal\\ length_i \\cap petal\\ width_i) = \\frac{P(Virginica)\\cdot P(sepal\\ length_i|Virginica) \\cdot P(sepal\\ width_i|Virginica) \\cdot P(petal\\ length_i|Virginica) \\cdot P(petal\\ width_i|Virginica)}{P(sepal\\ length_i \\cap sepal\\ width_i \\cap petal\\ length_i \\cap petal\\ width_i)}\n",
    "$$\n",
    "\n",
    "\n",
    "Based on whichever probability is the highest, we classify the observation *i* as one of the flower types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "# quick preview of the dataset\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target\n",
    "iris_df.head()\n",
    "\n",
    "# as we can see the target is denoted by an integer - 0 (setosa), 1 (versicolor), 2 (virginica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = iris.data, iris.target\n",
    "\n",
    "# split into training and test set (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=205)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and train the classifier\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred_sklearn = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating accuracy\n",
    "np.sum(y_pred_sklearn == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "      <th>predicted_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.5</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.5</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.2</td>\n",
       "      <td>4.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                 5.6               3.0                4.1               1.3   \n",
       "1                 5.9               3.2                4.8               1.8   \n",
       "2                 5.6               2.7                4.2               1.3   \n",
       "3                 5.5               4.2                1.4               0.2   \n",
       "4                 5.9               3.0                5.1               1.8   \n",
       "5                 7.1               3.0                5.9               2.1   \n",
       "6                 6.7               3.0                5.2               2.3   \n",
       "7                 6.7               3.1                4.4               1.4   \n",
       "8                 7.3               2.9                6.3               1.8   \n",
       "9                 6.1               2.6                5.6               1.4   \n",
       "10                6.5               2.8                4.6               1.5   \n",
       "11                5.8               2.7                5.1               1.9   \n",
       "12                7.7               2.8                6.7               2.0   \n",
       "13                6.8               2.8                4.8               1.4   \n",
       "14                6.0               2.2                5.0               1.5   \n",
       "15                5.7               2.8                4.1               1.3   \n",
       "16                6.4               3.1                5.5               1.8   \n",
       "17                4.9               3.6                1.4               0.1   \n",
       "18                5.1               3.5                1.4               0.3   \n",
       "19                6.3               2.3                4.4               1.3   \n",
       "20                5.0               3.5                1.3               0.3   \n",
       "21                5.4               3.7                1.5               0.2   \n",
       "22                5.7               2.6                3.5               1.0   \n",
       "23                5.6               2.9                3.6               1.3   \n",
       "24                5.2               4.1                1.5               0.1   \n",
       "25                6.5               3.0                5.5               1.8   \n",
       "26                6.0               3.0                4.8               1.8   \n",
       "27                5.1               3.3                1.7               0.5   \n",
       "28                5.0               3.4                1.6               0.4   \n",
       "29                5.5               2.4                3.8               1.1   \n",
       "\n",
       "    target  predicted_target  \n",
       "0        1                 1  \n",
       "1        1                 2  \n",
       "2        1                 1  \n",
       "3        0                 0  \n",
       "4        2                 2  \n",
       "5        2                 2  \n",
       "6        2                 2  \n",
       "7        1                 1  \n",
       "8        2                 2  \n",
       "9        2                 1  \n",
       "10       1                 1  \n",
       "11       2                 2  \n",
       "12       2                 2  \n",
       "13       1                 1  \n",
       "14       2                 1  \n",
       "15       1                 1  \n",
       "16       2                 2  \n",
       "17       0                 0  \n",
       "18       0                 0  \n",
       "19       1                 1  \n",
       "20       0                 0  \n",
       "21       0                 0  \n",
       "22       1                 1  \n",
       "23       1                 1  \n",
       "24       0                 0  \n",
       "25       2                 2  \n",
       "26       2                 2  \n",
       "27       0                 0  \n",
       "28       0                 0  \n",
       "29       1                 1  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing the predictions vs true values side by side\n",
    "test_set = pd.DataFrame(X_test, columns = iris.feature_names)\n",
    "test_set['target'] = y_test\n",
    "test_set['predicted_target'] = y_pred_sklearn\n",
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before directly implementing the 3 equations shown above, we can utilize some \"tricks\" to further simplify our problem and make the calculations simpler:\n",
    "1. As mentioned, we classify the observation to a certain flower type based on whichever of the 3 probabilities (given by the equations) is the highest. You can notice that all 3 equations have the same denominator, which is the joint probability of the features in the dataset. Since, for classification, we are only interested in comparing the values of the probabilities, with the goal being to find the highest one, and each equation shares the same denominator (i.e. they are divided by the same number in the end), we can accomplish our goal by also just directly comparing the numerators. As such, we only need to calculate those!\n",
    "\n",
    "2. To address a potential underflow problem (where as a result of multiplying a lot of small numbers we get to such a small number, the computer just assumes it's 0), we can calculate the $\\log$ of the probabilities instead. $\\log$ is a strictly increasing function, which preserves the maximum point, and therefore preserves the order of the probabilities after the application of it. This also has the added benefit of simplifying the product equation further since the $\\log$ of a product is the sum of $\\log$ 's, i.e.:\n",
    "$$\n",
    "\\log{(P(Setosa)\\cdot P(sepal\\ length_i|Setosa)...)} = \\log{P(Setosa)} + \\log{P(sepal\\ length_i|Setosa) + ...}\n",
    "$$\n",
    "\n",
    "3. Our features are all continous and we therefore cannot directly compute probabilities by counting the occurrence of a certain value for a feature by the total number of observations. We instead need to make an assumption about the probability distribution function of the continuous variable and base the conditional probability based on that. Here we will assume a normal/Gaussian probability distribution, as such:\n",
    "$$\n",
    "P(sepal\\ length_i|Setosa) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-{\\frac{(sepal\\ length_i - \\mu)^2}{2 \\sigma^2}}}\n",
    "$$\n",
    "\n",
    "$\\mu$ and $\\sigma$ here refer to the average sepal length and the standard deviation of the sepal length for observations belonging to the Setosa class respectively.\n",
    "\n",
    "Now we have everything ready and can start the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will create a class to match the behavior of sklearn implementation\n",
    "class MyNBClassifier:\n",
    "    # defining a fit function here similar to what the sci-kit learn classifier has - the point is to generate all the values that will be reused throughout the calculation of the posterior probabilities\n",
    "    def fit(self, X: np.array, y: np.array):\n",
    "        # defining our classes - 0, 1, 2 - np.unique also sorts them\n",
    "        self.classes = np.unique(y)\n",
    "        # creating a dictionary to store the means of each feature (sepal length etc.) per target class - e.g. for P(sepal length_i|Setosa) we need the average sepal length for the Setosa class - explained above\n",
    "        self.means = {}\n",
    "        # creating a dictionary to store the variances of each feature per target class - same as above, necessary for calculating the Gaussian PDF\n",
    "        self.variances = {}\n",
    "        # for each of the equations we need to multiply the conditional probabilities by the prior probabilities of each class - e.g. P(Setosa), P(Virginica), P(Versicolor) - storing them in dictionary here for easy access\n",
    "        self.priors = {}\n",
    "\n",
    "        for cl in self.classes:\n",
    "            X_cl = X[y == cl]\n",
    "            # calculating the means across X acis means we calculate mean per column (feature) - so this is a numpy array of shape p (number of features)\n",
    "            self.means[cl] = np.mean(X_cl, axis = 0)\n",
    "            # similarly calculating variances\n",
    "            self.variances[cl] = np.var(X_cl, axis = 0)\n",
    "            # now calculating priors - this simply \n",
    "            self.priors[cl] = len(X_cl)/len(X)\n",
    "\n",
    "    # for now just doing conditional probability for continuous variable, can expand to discrete random variables in the future\n",
    "    def log_gaussian_probability_given_class(self, feature: int, cls: int, X_values: np.array) -> np.array:\n",
    "        # feature_id = feature_map[feature]\n",
    "        # cls_id = target_map[cls]\n",
    "\n",
    "        # getting the relevant means, variances and priors\n",
    "        mu = self.means[cls][feature]\n",
    "        sigma_2 = self.variances[cls][feature]\n",
    "        # implementing the equation above\n",
    "        gaussian_probability = (1/np.sqrt(2*mu*sigma_2))*np.exp(-(X_values-mu)**2/(2*sigma_2))\n",
    "\n",
    "        return np.log(gaussian_probability)\n",
    "    \n",
    "    # now we bring it all together for the predictions \n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        # defining an empty numpy array here which has the desired shape of our output - joint log probability per observation and classs (n observations x n classes array)\n",
    "        joint_log_probabilities_per_class = np.empty((len(X), len(self.classes)))\n",
    "\n",
    "        # calculate the log joint probability of X and class cl using the equation we have from point 2 above\n",
    "        for cl in self.classes:\n",
    "            # defining the log prior - e.g. log P Setosa\n",
    "            log_prior = np.log(self.priors[cl])\n",
    "            # defining the log conditional probabilities through a loop over the features - e.g. P(sepal length_i|setosa), P(sepal width_i|setosa)\n",
    "            log_conditional_probabilities = np.zeros(X.shape[0])\n",
    "            for feature in range(X.shape[1]):\n",
    "                log_conditional_probabilities += self.log_gaussian_probability_given_class(feature, cl, X[:,feature])\n",
    "            \n",
    "            # storing them in our output array - each column of the array corresponds to a different class\n",
    "            joint_log_probabilities_per_class[:, cl] = log_prior + log_conditional_probabilities\n",
    "        \n",
    "        # return the index of the column with the highest value per row (= class in this case as well since every colum corresponds to different class)\n",
    "        return np.argmax(joint_log_probabilities_per_class, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and train the MY classifier\n",
    "my_nb = MyNBClassifier()\n",
    "my_nb.fit(X_train, y_train)\n",
    "# make predictions\n",
    "y_pred_myclassifier = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_pred_myclassifier == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no differences between my predictions and sklearn's classifier\n",
    "np.sum(y_pred_myclassifier != y_pred_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implemented classifier works as expected and matches the basic functionality of scikit-learn's Gaussian NB classifier. We could also add additional functionality to this to reverse calculate the actual probabilities per class but that will be left for later. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
